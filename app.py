import streamlit as st
import os
import sys
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

for key in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
    if key in os.environ:
        del os.environ[key]

st.set_page_config(page_title="Chatbot financial advisor", page_icon="ü§ñ")
st.title("Assistant IA - financial")

load_dotenv()
if not os.getenv("GOOGLE_API_KEY"):
    st.error(" Cl√© API manquante. V√©rifie ton fichier .env")
    st.stop()

with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    model_choice = st.selectbox(
        "Mod√®le", 
        ["gemini-pro", "gemini-2.5-flash-lite"], 
        index=0
    )
    
        
    default_prompt = """Tu es un Conseiller Financier Senior exp√©riment√© et prudent.
    Tes r√©ponses doivent √™tre :
    1. P√©dagogiques (explique les termes complexes comme ETF, PEA, Crypto).
    2. Prudentes (ajoute toujours un avertissement : "Ceci n'est pas un conseil en investissement certifi√©").
    3. Structur√©es (utilise des listes √† puces).

    Si on te pose une question hors de la finance (ex: cuisine), r√©ponds poliment que tu ne traites que les sujets financiers."""

    system_prompt = st.text_area("Personnalit√© du chatbot :", default_prompt, height=200)
        
        

    
    if st.button("üóëÔ∏è Effacer la conversation"):
        st.session_state.chat_history = []
        st.rerun()

llm = ChatGoogleGenerativeAI(
    model=model_choice,
    temperature=0.7,
    google_api_key=os.getenv("GOOGLE_API_KEY")
)

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []


for message in st.session_state.chat_history:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.markdown(message.content)

user_input = st.chat_input("Votre message ici...")

if user_input:
    with st.chat_message("user"):
        st.markdown(user_input)
    
    st.session_state.chat_history.append(HumanMessage(content=user_input))

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt), # La personnalit√© d√©finie dans la sidebar
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ])
    
    chain = prompt_template | llm

    with st.chat_message("assistant"):
        with st.spinner("L'IA r√©fl√©chit..."):
            try:
                response = chain.invoke({
                    "history": st.session_state.chat_history[:-1], 
                    "input": user_input
                })
                st.markdown(response.content)
                
                st.session_state.chat_history.append(AIMessage(content=response.content))
                
            except Exception as e:
                st.error(f"Erreur API : {e}")